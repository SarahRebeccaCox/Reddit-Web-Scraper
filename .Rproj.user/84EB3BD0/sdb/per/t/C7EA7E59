{
    "contents" : "#library(lasso2)\n#library(tm)           # Framework for text mining.\n#library(SnowballC)    # Provides wordStem() for stemming.\n#library(qdap)         # Quantitative discourse analysis of transcripts.\n#library(qdapDictionaries)\n#library(dplyr)        # Data preparation and pipes %>%.\nlibrary(rjson)\nlibrary(RJSONIO)\nlibrary(rvest, warn.conflicts=FALSE)\nlibrary(RSelenium)\n#library(jsonlite)\n\n\n###################################\n#Getting comments through the JSON#\n###################################\n\n#the main URL to pull from\nurl <- 'https://www.reddit.com/r/politics/'\n\n\n#open the page\nwait_till_page_load<-function(page_load_time_out=60){\n  t0<-Sys.time()\n  while(browser$executeScript(\"return document.readyState;\")[[1]]!=\"complete\" & (Sys.time()-t0)<=page_load_time_out){\n    Sys.sleep(0.5)\n  }\n  invisible(0)\n}\n\nopen.page <- function(url){\n  checkForServer() # check if Selenium Server package is installed and if not, install now\n  startServer(invisible = FALSE, log = FALSE) # start server\n  browser <- remoteDriver(remoteServerAddr = \"localhost\", port = 4444, browserName = \"firefox\")\n  \n  \n  browser$open()\n  browser$navigate(url)\n  #wait_till_page_load(500000000)\n  return(browser)\n}\n\n\n\n#create thread list\ncreate.thread.list <- function(browser){ #works from open browser\n  links <- browser$findElements(using=\"css selector\", value=\".may-blank\")\n  urls <- rep(NA, length(links))\n  \n  urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))\n  \n  #only get the comment urls\n  url.list <- urls[grep(\"r/politics/comments\",urls)]\n  \n  for (i in 1:length(url.list)) {\n    url.list[i] <- paste0(url.list[i],\".json\")\n  }\n  \n  return(url.list)\n} \n\n\n\n#get the \"next\" link to crawl later\nget.next.link <- function(browser){\n  links.next <- browser$findElements(using=\"css selector\", value=\".nextprev a\")\n  urls.next <- rep(NA, length(links.next))\n  \n  urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))\n  next.link <- urls.next[grep(\"?count=25\",urls.next)]\n  \n  return(next.link)\n}\n\n\n#get the \"next\" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE\nget.next.link2 <- function(browser){\n  links.next <- browser$findElements(using=\"css selector\", value=\"#siteTable .separator+ a\")\n  urls.next <- rep(NA, length(links.next))\n  \n  urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))\n  next.link <- urls.next[grep(\"?count=\",urls.next)]\n  \n  return(next.link)\n}\n\n\nmain.data.generator <- function(url.list,i){\n  #select the URL of the thread to scrape from\n  url.comment <- as.character(url.list[i]) \n  \n  #convert from JSON to R-usable data\n  rawdat <- rjson::fromJSON(file=url.comment)\n  \n  #pulling just want we want\n  main.data <- rawdat[[2]]$data$children\n  \n  return(main.data)\n  \n  #main.data[[5]]$data$body #comment number 5\n  #main.data[[5]]$data$author #author of comment number 5\n  #information about the parent post title and url can be found in rawdat[[1]]\n}\n\n\n\n#this function puts comments and their authors into a matrix from a given main.data\ncomments.to.dataframe <- function(main.data){\n  \n  #initialize a matrix\n  data.list <- matrix(c(main.data[[1]]$data$created_utc,main.data[[1]]$data$body),nrow=1)\n  \n  if (length(main.data) != 1){\n    for (i in 2:(length(main.data)-1)){\n      data.list <- rbind(data.list,c(main.data[[i]]$data$created_utc,main.data[[i]]$data$body))\n    }\n  }\n  return(data.list)\n}\n\n\nautomate.scraping <- function(number.of.pages,matrix){\n  i <- 1\n  while (i<=number.of.pages){\n    url.list <- create.thread.list(browser)\n    \n    for (url in 1:length(url.list)){ #for every thread\n      main.data <- main.data.generator(url.list,url) #create main data\n      \n      if (length(main.data) == 0){\n        next\n      }\n      if (length(main.data) != 0){\n        matrix <- rbind(matrix,comments.to.dataframe(main.data))\n      }\n    }\n    \n    next.link.2 <- get.next.link2(browser)\n    browser$navigate(next.link.2)\n    \n    print(paste0(as.character(round(100*i/33)),\"%\"))\n    i <- i+1\n    \n  }\n  return(matrix)\n}\n\n\n####################\n#USING THIS CRAWLER#\n####################\n\n#Steps for when all the functions are defined. \n\n#STEP 1: PUT IN THE URL AND OPEN THE BROWSER\n#the main URL to pull from\nurl <- 'https://www.reddit.com/r/politics/'\nbrowser <- open.page(url)\n\n#STEP 2: GET URLS TO THREADS ON PAGE\nurl.list <- create.thread.list(browser)\n\n\n#STEP 3: GET LINK TO CLICK TO NEXT PAGE\nnext.link <- get.next.link(browser)\n\n#STEP 4: INITIALIZE DATA MATRIX\ndata.matrix <- matrix(c(\"a\",\"b\"),nrow=1)\n\n#STEP 5: GATHER COMMENT DATA FROM EACH THREAD\nfor (url in 1:length(url.list)){ #for every thread\n  main.data <- main.data.generator(url.list,url) #create main data\n  \n  if (length(main.data) == 0){\n    next\n  }\n  else{\n    data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))\n  }\n}\n\n\n#STEP 6: NAVIGATE TO THE NEXT PAGE\nbrowser$navigate(next.link) \n\n#STEP 7: COLLECT COMMENT DATA ON NEW PAGE (by repeating steps 4+5)\nurl.list <- create.thread.list(browser)\n\nfor (url in 1:length(url.list)){ #for every thread\n  main.data <- main.data.generator(url.list,url) #create main data\n  \n  if (length(main.data) == 0){\n    next\n  }\n  else{\n    data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))\n  }\n}\n\n#STEP 8: GO TO THE NEXT PAGE USING THE SECOND \"NEXTLINK\" FUNCTION AND GO TO NEXT PAGE\nnext.link.2 <- get.next.link2(browser)\nbrowser$navigate(next.link.2)\n\n\n#STEP 9: REPEAT STEPS 7 AND 8 FOR AS LONG AS YOU LIKE\n\n## number.of.pages is the number of additional pages you would like to scroll through and take data from.\n## for example, automate.scraping(3) will repeat steps 7 and 8 3 times.\ndata.matrix <- automate.scraping(33,data.matrix)\n\n\n#STEP 10: CONVERT YOUR DATA INTO A DATAFRAME\nreddit.data <- as.data.frame(data.matrix)\nreddit.data <- reddit.data[-1,]\nnames(reddit.data) <- c(\"UTC\",\"Body\")\n\n#STEP 11: SAVE TO CSV\nwrite.csv(reddit.data,\"RedditDataApr182.csv\")\n\n\n\n#DON'T USE BELOW HERE\n\n\n###############################################\n#ATTEMPT TO GET ALL COMMENTS VIA SELECTOR TOOL#\n###############################################\n\n#this method returns more comments (and subcomments!)\n#but we can't easily map authors to comments\n#and we have to clean out html\n\nbrowser$navigate(url.list[5])\ncomments <- browser$findElements(using=\"css selector\", value=\".md p\")\n\n\ncomment.url <- url.list[5]\ncomments <- read_html(comment.url) # reading the HTML code\ntext <- html_nodes(comments, \".md\") # identify the CSS selector\ntextauth <- html_nodes(comments, \".may-blank\")\ntext # content of CSS selector\ntext2 <- as.character(text)\ntextauth2 <- as.character(textauth)\n#this method will require cleaning out HTML, but I think it's faster\n\n##########################################\n\n\n#this section will eventually be able to click the \"load more comments\" button and continue scraping, hopefully. Ignore for now.\n\n\n##########################################\n\n\n",
    "created" : 1457655075072.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "415404298",
    "id" : "C7EA7E59",
    "lastKnownWriteTime" : 1461008346,
    "path" : "~/Documents/Coursework/NYU - MS/Spring2016/Big Data/Project/Reddit Web Scraper/Reddit Web Scraper.R",
    "project_path" : "Reddit Web Scraper.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}