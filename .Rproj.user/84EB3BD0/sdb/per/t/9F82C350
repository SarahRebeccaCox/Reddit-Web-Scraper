{
    "contents" : "---\ntitle: \"Scraping data behind web forms\"\nauthor: \"Pablo Barbera\"\ndate: \"January 22, 2016\"\noutput: html_document\n---\n\n### Scraping web data behind web forms\n\nThe most difficult scenario for web scraping is when data is hidden behind multiple pages that can only be accessed entering information into web forms. There are a few approaches that might work in these cases, with varying degree of difficulty and reliability, but in my experience the best method is to use [Selenium](https://en.wikipedia.org/wiki/Selenium_(software)).\n\nSelenium automates web browsing sessions, and was originally designed for testing purposes. You can simulate clicks, enter information into web forms, add some waiting time between clicks, etc.\n\nTo learn how it works, we will scrape the website _Monitor Legislativo_, which provides information about the candidates in the recent Venezuelan legislative elections.\n\n```{r}\nurl <- 'http://eligetucandidato.org/filtro/'\n```\n\nAs you can see, the information we want to scrape is hidden behind these two selectors. Let's see how we can use Selenium to scrape it.\n\nThe first step is to install it and load it.\n```{r}\n# install.packages(\"RSelenium\")\nlibrary(RSelenium)\ncheckForServer() # check if Selenium Server package is installed and if not, install now\nstartServer() # start server\nbrowser <- remoteDriver(remoteServerAddr = \"localhost\", port = 4444, browserName = \"firefox\")\n```\n\nAnd now we can open an instance of Firefox, and navigate to the url:\n\n```{r}\nbrowser$open()\nbrowser$navigate(url)\n```\n\nLet's assume we want to see the results of the state of Distrito Capital for the GPP party. First, let's open the source code to try to identify the elements that we're trying to scrape. With Firefox, you can use _Tools_ > _Web Developer_ > _Inspector_ and then click on the list of states and parties to find this information.\n\n```{r}\nstate <- browser$findElement(using = 'id', value=\"estado\")\nstate$sendKeysToElement(list(\"Distrito Capital\"))\nparty <- browser$findElement(using = 'id', value=\"partido\")\nparty$sendKeysToElement(list(\"GPP\"))\n```\n\nFinally, let's find the information for the _Send_ button and click on it.\n\n```{r}\nsend <- browser$findElement(using = 'id', value=\"enviar\")\nsend$clickElement()\n```\n\nFrom this website, we want to scrape the URLs to each candidate's page. Note that there are some duplicates, which we will need to clean. As in the previous cases, we can use selectorGadget to help us identify the information we want to extract.\n\n```{r}\nlinks <- browser$findElements(using=\"css selector\", value=\".portfolio_title a\")\nurls <- rep(NA, length(links))\nfor (i in 1:length(links)){\n    urls[i] <- unlist(links[[i]]$getElementAttribute('href'))\n}\n# alternatively, we can also do\nurls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))\n# removing duplicates\nurls <- urls[!duplicated(urls)]\nurls\n```\n\nThe final step would be to scrape the information from each of these candidate websites, and do all of this inside loops over states and parties, and then candidates. I will not cover the rest of the example in class, but here's the code in case you want to use parts of it for your projects:\n\n```{r, eval=FALSE}\n############################################################\n## first, we scrape the list of districts and parties\n############################################################\n\nurl <- 'http://eligetucandidato.org/filtro/'\ntxt <- readLines(url)\n\nestados <- txt[288:311]\nestados <- gsub(\".*>(.*)<.*\", estados, repl=\"\\\\1\")\n\npartidos <- txt[315:317]\npartidos <- gsub(\".*>(.*)<.*\", partidos, repl=\"\\\\1\")\n\n############################################################\n## now, loop over pages to extract the URL for each candidate\n############################################################\n\nlibrary(RSelenium)\nRSelenium::checkForServer()\nRSelenium::startServer()\nbrowser <- remoteDriver(remoteServerAddr = \"localhost\" \n                     \t, port = 4444\n                     \t, browserName = \"firefox\"\n                     \t)\nbrowser$open()\n\ncandidatos <- c()\n\nfor (estado in estados){\n\tmessage(estado)\n\tfor (partido in partidos){\n\t\tmessage(partido)\n\n\t\tbrowser$navigate(url)\n\t\t# input: estado\n\t\tSys.sleep(1)\n\t\tmore <- browser$findElement(using = 'id', value=\"estado\")\n\t\tmore$sendKeysToElement(list(estado))\n\t\t# input: partido\n\t\tSys.sleep(1)\n\t\tmore <- browser$findElement(using = 'id', value=\"partido\")\n\t\tmore$sendKeysToElement(list(partido))\n\t\t# click on \"buscar\"\n\t\tmore <- browser$findElement(using = 'id', value=\"enviar\")\n\t\tmore$clickElement()\n\t\t# extracting URLS\n\t\tmore <- browser$findElements(using = 'xpath', value=\"//a[@target='_self']\")\n\t\tcount <- 0\n\t\twhile (length(more)<2 & count < 10){\n\t\t\tSys.sleep(1)\n\t\t\tmore <- browser$findElements(using = 'xpath', value=\"//a[@target='_self']\")\n\t\t\tcount <- count + 1\n\t\t}\n\t\turls <- unlist(sapply(more, function(x) x$getElementAttribute('href')))\n\t\turls <- unique(urls)\n\n\t\tcandidatos <- c(candidatos, urls)\n\t\tmessage(length(urls), ' candidatos nuevos, ', length(candidatos), ' en total')\n\n\n\t}\n}\n\nwriteLines(candidatos, con=file(\"candidatos-urls.txt\"))\n\n############################################################\n## download the html code of candidates' URLS\n############################################################\n\nfor (url in candidatos){\n\tid <- gsub('.*id=', '', url)\n\tfilename <- paste0('html/', id, '.html')\n\tif (file.exists(filename)){ next }\n\tmessage(url)\n\thtml <- readLines(url)\n\twriteLines(html, con=file(filename))\n\tSys.sleep(2)\n}\n\n############################################################\n## extract information from html\n############################################################\n\nfls <- list.files(\"html\", full.names=TRUE)\ndf <- list()\n\nfor (i in 1:length(fls)){\n\n\ttxt <- readLines(fls[i])\n\n\t# id\n\tid <- gsub('html/(.*).html', fls[i], repl=\"\\\\1\")\n\t# name\n\tname <- txt[grep(\"displayCenter vc_single_image\", txt)]\n\tname <- gsub(\".*title=\\\"(.*)\\\">.*\", name, repl='\\\\1')\n\t# partido\n\tpartido <- txt[grep(\"Partido Político\", txt)]\n\tpartido <- gsub('.*</strong> (.*)</span.*', partido, repl=\"\\\\1\")\n\t# edad\n\tedad <- txt[grep(\"Edad\", txt)]\n\tedad <- gsub('.*ong>(.*)</span.*', edad, repl=\"\\\\1\")\n\t# circuito\n\tcircuito <- txt[grep(\"Circuito\", txt)]\n\tcircuito <- gsub('.*Circuito (.*)</span.*', circuito, repl=\"\\\\1\")[1]\n\t# estado\n\testado <- txt[grep(\"Circuito\", txt)]\n\testado <- gsub('.*\\\">(.*) –&nbsp;.*', estado, repl=\"\\\\1\")[1]\n\t# twitter\n\ttwitter <- txt[grep(\"fa-twitter.*@\", txt)]\n\ttwitter <- gsub('.*@(.*)</spa.*', twitter, repl=\"\\\\1\")\n\tif (length(twitter)==0){ twitter <- NA }\n\n\tdf[[i]] <- data.frame(\n\t\tid = id, name = name, partido = partido, edad = edad,\n\t\tcircuito = circuito, estado = estado, twitter = twitter,\n\t\tstringsAsFactors=F)\n}\n\ndf <- do.call(rbind, df)\ndf <- df[order(df$estado, df$partido, df$circuito),]\n\nwrite.csv(df, file=\"venezuela-monitor-legislativo-data.csv\",\n\trow.names=FALSE)\n\n```\n\n\n\n\n",
    "created" : 1457655579142.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2843042894",
    "id" : "9F82C350",
    "lastKnownWriteTime" : 1453491198,
    "path" : "~/Desktop/data-science-workshop-master 3/scraping/03_scraping-advanced.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}