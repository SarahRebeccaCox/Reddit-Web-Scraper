{
    "contents" : "---\noutput: html_document\n---\n---\ntitle: \"Survey Bias Among Video Gaming Redditors\"\noutput: html_document\n--- \n\nIf you've ever taken a survey like the U.S. Census Survey, you know that they can be very long and difficult to answer completely honestly. In the case of the Census survey, this may be in part because the survey is very long and you have better things to do with your day than list every single item you've purchased in the last three months. <b>However, other surveys may probe you with questions that make you uncomfortable, or questions that you may alter your answer to depending who is asking. This is called social desirability response bias, and it may or may not be an issue for researchers relying heavily on survey data.</b>\n\nSchneider and Buckley (2002) conducted a study evaluating school choice for parents in the Washington D.C. area. Survey results were compared to \"revealed preferences\" measured by online school search behavior. Findings included some differences in the importance of things like teacher quality between survey results and search behavior.\n\n<b>This study aims to assess social desirability response bias among \"Redditors\" (users of the website reddit.com) who play video games or work in the video game industry.</b> Gamergate is the name given to the discussion and manifestation of sexism in video game culture, especially in the video game industry. A campaign of misogynist attacks targeted at specific female game developers and feminist culture critics was coordinated on the forums of Reddit, 4chan, and 8chan. Reddit users post on the forums with usernames, while 4chan and 8chan are completely anonymous. This made Reddit the best choice to use for this project.\n\nUsers in the population of interest were given a survey measuring two kinds of sexism, benevolent and hostile. Users had the option of providing their username in the survey. Then, text mining and sentiment analysis will be used to assess \"sexist behavior\" of users who gave their username via their previous posts on the website, and also to assess \"general sexism\" across the video gaming subreddit (under the assumption that survey respondents are a random sample of users on this subreddit).\n\n```{r, echo=FALSE, warning=FALSE}\n\nlibrary(lattice)\nlibrary(knitr)\nlibrary(png)\n\n```\n\n```{r, echo=FALSE,message=FALSE,error=FALSE,warning=FALSE}\n###LOAD DATA\naverage.scores <- read.csv(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/AverageScores.csv\")\n\naverage.scores <- average.scores[,-1]\nsetwd(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/\")\n```\n\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/SurveyScreen.png\">\n\nWithin hours, Redditors were already starting to lash out: \n\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/Comments.png\">\n\n74 responses were gathered before the survey was closed. Of those:\n\n21 identified as female, 50 as male, and 3 as something outside the binary,\n\n7 work in the video game industry,\n\nand 30 provided usernames.\n\nThe Ambivalent Sexism Inventory consists of 22 items and measures on Benevolent Sexism (\"knight in shining armor\" ideology, protects women who conform to traditional gender roles) and Hostile Sexism (negative feelings toward women). Each scale is scored from 0 to 5, where lower scores indicate lower levels of sexism as defined by the inventory.\n\n```{r, echo=FALSE, fig.height=3}\n###Overall scores\nbarchart(average.scores[15:16,2]~as.numeric(average.scores[15:16,3]),groups=average.scores[15:16,1],scales=list(x=list(rot=0,cex=0.8)),xlab=\"Scores\",main=\"Sexism Scores Overall\",auto.key=TRUE,xlim=c(0,5))\n\n```\n\n```{r, echo=FALSE, fig.height=3}\n###By Gender\nbarchart(average.scores[1:6,2]~as.numeric(average.scores[1:6,3]),groups=average.scores[1:6,1],scales=list(x=list(rot=0,cex=0.8)),xlab=\"Scores\",main=\"Sexism Scores by Gender\",auto.key=TRUE,xlim=c(0,5))\n\n```\n\n```{r, echo=FALSE, fig.height=3}\n###By Username Status\nbarchart(average.scores[7:10,2]~as.numeric(average.scores[7:10,3]),groups=average.scores[7:10,1],scales=list(x=list(rot=0,cex=0.8)),xlab=\"Scores\",main=\"Sexism Scores by Username Status\",auto.key=TRUE,xlim=c(0,5))\n```\n\n``````{r, echo=FALSE, fig.height=3}\n###By Industry Status\nbarchart(average.scores[11:14,2]~as.numeric(average.scores[11:14,3]),groups=average.scores[11:14,1],scales=list(x=list(rot=0,cex=0.8)),xlab=\"Scores\",main=\"Sexism Scores by Industry Status\",auto.key=TRUE,xlim=c(0,5))\n```\n\nWhen you consider that the maximum score on each scale is 5 and the average scores for US respondents overall are closer to 2 and 3 (for women and men, respectively, according to the ASI website), this is actually not so bad! But let's take a look at the message boards:\n\nA quick search of \"girls\" into the video games reddit subforum gives us this:\n\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/RedditGirls.png\">\n\nUh oh! Looks pretty bad. But how do I decide how bad? \n\nReddit has a nice API:\n\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/JSON.png\">\n\nUsing this API, words and phrases can be extracted and ultimately analyzed to see if their sexism levels match up with the survey results. \n\nAs a first look, let's scrape all the posts from each user who provided a username. You can access this information on the web by typing \"http://www.reddit.com/user/USERNAME/comments/\". Let's look at mine:\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/MyRedditComments.png\">\n\nFirst, I'll set up the data and get the necessary libraries:\n\n```{r}\n### Get the list of usernames\nresponses <- read.csv(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/CleanedResponses.csv\")\nusername.field <- data.frame(responses[,4])\n```\n```{r,warning=FALSE,message=FALSE}\n### Get necessary libraries\noptions(scipen=999)\nlibrary(lasso2)\nlibrary(tm)           # Framework for text mining.\nlibrary(SnowballC)    # Provides wordStem() for stemming.\nlibrary(qdap)         # Quantitative discourse analysis of transcripts.\nlibrary(qdapDictionaries)\nlibrary(dplyr)        # Data preparation and pipes %>%.\nlibrary(RJSONIO)\nlibrary(jsonlite)\n\n```\n```{r,echo=FALSE}\nsetwd(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/Usernames\")\n```\n\nNext, I can create a list of usernames, and from that create a list of JSON URLs to extract text from:\n\n```{r}\n\n### Extract the list of non-blank usernames\nn <- nrow(username.field)\nusername.list <- list()\nfor (i in 1:n) {\n  username <- toString(username.field[i,])\n\tif (username != \"\") {\n\t\tusername.list <- rbind(username.list,username)\n\t}\n}\n\n### Create list of URLs\nn <- nrow(username.list)\nurl.list <- list()\nfor (i in 1:n) {\n\turl <- paste0(\"http://www.reddit.com/user/\",username.list[i,],\"/comments/.json\")\n\turl.list <- rbind(url.list,url)\n}\n\n```\n\nI'm only interested in user comments, so I'll create a data frame containing each user's comments:\n\n```{r, warning=FALSE}\n### The following code creates a data frame of user comments\nurl <- as.character(url.list[1])\nrawdat <- fromJSON(readLines(url, warn = FALSE))\nmain.node <- rawdat$data$children$data$body\n\nfor (i in 2:n) {\n  url <- as.character(url.list[i])\n\trawdat <- fromJSON(readLines(url, warn = FALSE))\n\tmain.2 <- rawdat$data$children$data$body\n\tmain.node <- cbind(main.node,main.2)\n}\ncolnames(main.node) <- username.list\n```\n\nEach column corresponds to a particular username. Now we have a really nice set of readable comments (take a look at comment number three!):\n\n\n```{r}\nhead(main.node[,2])\n```\n\nMy text-mining code works by importing text files, so I'll write each column of data into a text file.\n\n```{r}\nfor (i in 1:n) {\n  write(main.node[,i],file = as.character(username.list[i]))\n}\n```\n\nHere are the text files, each named by username: \n\n```{r,echo=FALSE}\nsetwd(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles\")\n```\n\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/TextFiles.png\">\n\n```{r,echo=FALSE}\nsetwd(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project//Usernames\")\n```\n\nNow we can begin the actual text mining. The first step is to build the corpus, a body of documents to pull text from.\n\n```{r}\n\n#############\n#TEXT MINING#\n#############\n\n### Build the corpus   \n\n\ncname <- file.path(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/Usernames\") #where files come from\ncname\nlength(dir(cname))  #Number of documents\ndir(cname)          #Check the filenames\n\ndocs <- Corpus(DirSource(cname))\ndocs #30 documents, it worked correctly\n```\n\nIf we take a look at one of the documents, say Document 11, we see the following: \n\n```{r,echo=FALSE}\nsetwd(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles\")\n```\n\n<img src=\"/Users/sarahcox/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/HTMLNeccessaryFiles/UnprocessedText3.png\">\n\n```{r,echo=FALSE}\nsetwd(\"~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/Usernames\")\n```\n\nThis text is readable, but not so easy to analyze.\n\nWe can use the following transformations to help clean this up:\n\n```{r}\n### Preprocessing\ngetTransformations()\n\n#Replace @ and | with a space\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\ndocs <- tm_map(docs, toSpace, \"/|@|\\\\|\")\n\n# Replace \"ampquot\" with space\ndocs <- tm_map(docs, toSpace, \"ampquot\")\n\n# Conversion to Lower Case\ndocs <- tm_map(docs, content_transformer(tolower))\n\n# Remove Numbers\ndocs <- tm_map(docs, removeNumbers)\n\n# Remove Punctuation\ndocs <- tm_map(docs, removePunctuation)\n\n# Remove stop words  (ie for, very, and, of, are, ...)\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\n\n# Remove Single Letters\ndocs <- tm_map(docs, removeWords, letters)\n\n# Remove Whitespace\ndocs <- tm_map(docs, stripWhitespace)\n\n# Stemming\ndocs <- tm_map(docs, stemDocument)\n\n# Remove Single Letters\ndocs <- tm_map(docs, removeWords, letters)\n\n# Remove Whitespace\ndocs <- tm_map(docs, stripWhitespace)\n```\n\nFinally, we can see what we have to work with: \n\n```{r}\ninspect(docs[11])\n```\n\nThis is usable data. Let's inspect it a little bit:\n\nA document-term matrix is a matrix of term frequency in a collection of documents, where rows correspond to documents and columns correspond to terms. Let's create one:\n\n```{r}\n# Creating a Document Term Matrix\ndtm <- DocumentTermMatrix(docs)\ndim(dtm)\n\n```\n\nWe can get the term frequencies by converting dtm into a matrix and summing column counts:\n```{r}\nfreq <- colSums(as.matrix(dtm))\nlength(freq)\nfreq[100:105]\n```\n\nIf we want to see the most frequent terms, we can order freq in reverse:\n\n```{r}\nord <- order(freq)\nfreq[rev(ord)][1:20]\n```\n\nThis isn't surprising or very interesting.\n\nWhat might be more interesting is word associations. \n\n```{r}\nfindAssocs(dtm, \"woman\", corlimit=0.8)\nfindAssocs(dtm, \"gender\", corlimit=0.8)\nfindAssocs(dtm, \"women\", corlimit=0.8)\nfindAssocs(dtm, \"girl\", corlimit=0.8)\n```\n\nFor quantitative analysis on these words, we can convert the term-document matrix into a matrix and retain only those words that are less than ten characters:\n\n\n```{r}\nwords <- dtm %>%\n  as.matrix %>%\n  colnames %>%\n  (function(x) x[nchar(x) < 10])\nlength(words)\ntypeof(words)\n```\n\n\"words\" is now a character vector of 3397 words. \n\nThe closest to a sentiment analysis I was able to achieve utilizes a function in the library \"qdap\". The function, polarity(), outputs an average polarity score (polarity divided by number of words), the standard deviation, and a standardized polarity that is the average polarity divided by the standard deviation. \n\n```{r}\npolarity(docs[1])\n```\n\nThere is an argument within this function that allows for a custom polarity frame of terms and polarity weights, however I could not figure out what words or weights to use in a polarity frame for sexism or how to make it work with the function in time.\n\n```{r}\nqdapDictionaries::key.pol\n```\n\nThe idea would be to create my own data frame composed of words from my own word set and scoring them based on sexism.\n\n```{r}\nhead(words)\n```\n\n\nFuture work:\n\nDevelop or find a measure of sexism to implement sentiment analysis with, preferably in context of words/sentences. \n\nDefine a baseline sexism level for all reddit users.\n\nGet a larger sample size, ideally with a more subtle sexism inventory with no subscores.\n\nMore efficient text-cleaning.\n\nFind out if there's a way to extract all usernames of survey respondents, even those who chose to remain anonymous.",
    "created" : 1457660111898.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3684051267",
    "id" : "ED9A0BED",
    "lastKnownWriteTime" : 1457400403,
    "path" : "~/Documents/Coursework/NYU - MS/Spring2015/Education DS/GitVersion/Education-Data-Science-Project/SarahCoxProject.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}