data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url
url
url.list[1]
url.comment <- as.character(url.list[i])
i <- 1
url.comment <- as.character(url.list[i])
rawdat <- fromJSON(file=url.comment)
i <- 5
url.comment <- as.character(url.list[i])
rawdat <- fromJSON(file=url.comment)
url.comment
?fromJSON
url.comment
rawdat <- fromJSON(file=url.comment)
library(jsonlite)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url.comment
rawdat <- fromJSON(file=url.comment)
?fromJSON
library(rjson)
rawdat <- fromJSON(file=url.comment)
library(rjson)
rawdat <- fromJSON(file=url.comment)
#library(tm)           # Framework for text mining.
#library(SnowballC)    # Provides wordStem() for stemming.
#library(qdap)         # Quantitative discourse analysis of transcripts.
#library(qdapDictionaries)
#library(dplyr)        # Data preparation and pipes %>%.
library(rjson)
#library(RJSONIO)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
#library(jsonlite)
url <- 'https://www.reddit.com/r/politics/'
#open the page
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
for (i in 1:length(url.list)) {
url.list[i] <- paste0(url.list[i],".json")
}
return(url.list)
}
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls)]
return(next.link)
}
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls)]
return(next.link)
}
main.data.generator <- function(url.list,i){
#select the URL of the thread to scrape from
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
return(main.data)
#main.data[[5]]$data$body #comment number 5
#main.data[[5]]$data$author #author of comment number 5
#information about the parent post title and url can be found in rawdat[[1]]
}
#this function puts comments and their authors into a matrix from a given main.data
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
#add the other rows
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
return(data.list)
}
url <- 'https://www.reddit.com/r/politics/'
browser <- open.page(url)
#STEP 2: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#STEP 2: GET URLS TO THREADS ON PAGE
url.list <- create.thread.list(browser)
#STEP 4: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls.next)]
return(next.link)
}
next.link <- get.next.link(browser)
data.matrix <- matrix(c("a","b"),nrow=1)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url.list[1]
i <- 6
url.list[i]
url.comment <- as.character(url.list[i])
rawdat <- fromJSON(file=url.comment)
sessionInfo()
library(rjson)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
url <- 'https://www.reddit.com/r/politics/'
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
for (i in 1:length(url.list)) {
url.list[i] <- paste0(url.list[i],".json")
}
return(url.list)
}
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls.next)]
return(next.link)
}
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls)]
return(next.link)
}
main.data.generator <- function(url.list,i){
#select the URL of the thread to scrape from
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
return(main.data)
#main.data[[5]]$data$body #comment number 5
#main.data[[5]]$data$author #author of comment number 5
#information about the parent post title and url can be found in rawdat[[1]]
}
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
#add the other rows
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
return(data.list)
}
browser <- open.page(url)
url.list <- create.thread.list(browser)
#STEP 3: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#STEP 4: INITIALIZE DATA MATRIX
data.matrix <- matrix(c("a","b"),nrow=1)
url.list
i <- 6
url
url <- 6
main.data <- main.data.generator(url.list,url)
browser$navigate(url.list[6])
url.comment <- as.character(url.list[i])
url.comment
#convert from JSON to R-usable data
rawdat <- fromJSON(file=url.comment)
rawdat <- fromJSON(file="https://www.reddit.com/r/politics/comments/49ue0a/hillary_clinton_forgot_that_she_supported_a_coup/.json")
?fromJSON
main.data <- rawdat[[2]]$data$children
rawdat <- fromJSON(file=url.comment)
fromJSON
rawdat <- fromJSON(file="https://www.reddit.com/r/politics/comments/49ue0a/hillary_clinton_forgot_that_she_supported_a_coup/")
library(lasso2)
library(tm)           # Framework for text mining.
library(SnowballC)    # Provides wordStem() for stemming.
library(qdap)         # Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr)        # Data preparation and pipes %>%.
library(rjson)
#library(RJSONIO)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
#library(jsonlite)
rawdat <- fromJSON(file=url.comment)
library(RJSONIO)
rawdat <- fromJSON(file=url.comment)
?fromJSON
rawdat <- fromJSON(file=url.list[6])
rawdat <- rsjon::fromJSON(file=url.comment)
rawdat <- rjson::fromJSON(file=url.comment)
url.list[5]
url.list[4]
url.list[8]
url.list[9]
url.list[11]
url.list[12]
url.comment <- as.character(url.list[12])
rawdat <- rjson::fromJSON(file=url.comment)
url.comment
main.data <- main.data.generator(url.list,url) #create main data
main.data.generator <- function(url.list,i){
#select the URL of the thread to scrape from
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- rjson::fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
return(main.data)
#main.data[[5]]$data$body #comment number 5
#main.data[[5]]$data$author #author of comment number 5
#information about the parent post title and url can be found in rawdat[[1]]
}
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
dim(data.matrix)
browser$navigate(next.link)
url.list <- create.thread.list(browser)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
next.link.2 <- get.next.link2(browser)
next.link.2 <- get.next.link2(browser)
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls.next)]
return(next.link)
}
next.link.2 <- get.next.link2(browser)
browser$navigate(next.link.2)
dim(data.matrix)
url.list <- create.thread.list(browser)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
i
main.data[[2]]
main.data
length(main.data)
library(lasso2)
library(tm)           # Framework for text mining.
library(SnowballC)    # Provides wordStem() for stemming.
library(qdap)         # Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr)        # Data preparation and pipes %>%.
library(rjson)
library(RJSONIO)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
#library(jsonlite)
url <- 'https://www.reddit.com/r/politics/'
#open the page
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
for (i in 1:length(url.list)) {
url.list[i] <- paste0(url.list[i],".json")
}
return(url.list)
}
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls.next)]
return(next.link)
}
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls.next)]
return(next.link)
}
main.data.generator <- function(url.list,i){
#select the URL of the thread to scrape from
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- rjson::fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
return(main.data)
#main.data[[5]]$data$body #comment number 5
#main.data[[5]]$data$author #author of comment number 5
#information about the parent post title and url can be found in rawdat[[1]]
}
#this function puts comments and their authors into a matrix from a given main.data
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
#add the other rows
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
return(data.list)
}
url <- 'https://www.reddit.com/r/politics/'
browser <- open.page(url)
url.list <- create.thread.list(browser)
#STEP 3: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#STEP 4: INITIALIZE DATA MATRIX
data.matrix <- matrix(c("a","b"),nrow=1)
#STEP 5: GATHER COMMENT DATA FROM EACH THREAD
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
browser$navigate(next.link)
#STEP 7: COLLECT COMMENT DATA ON NEW PAGE (by repeating steps 4+5)
url.list <- create.thread.list(browser)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
i
length(main.data)
url.list
i <- 1
url.comment <- as.character(url.list[i])
rawdat <- rjson::fromJSON(file=url.comment)
main.data <- rawdat[[2]]$data$children
i <- 2
url.comment <- as.character(url.list[i])
rawdat <- rjson::fromJSON(file=url.comment)
main.data <- rawdat[[2]]$data$children
length(url.list)
i <- 27
url.comment <- as.character(url.list[i])
rawdat <- rjson::fromJSON(file=url.comment)
main.data <- rawdat[[2]]$data$children
length(main.data)
i <- 3
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- rjson::fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
for (i in 1:length(url.list)){
main.data.generator(url.list,i)
}
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
if (length(main.data)==1){
next
}
else{
#add the other rows
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
}
return(data.list)
}
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
i <- 26
main.data.generator(url.list,26)
length(main.data.generator(url.list,26))
length(main.data.generator(url.list,25))
length(main.data.generator(url.list,24))
length(main.data.generator(url.list,23))
length(main.data.generator(url.list,22))
length(main.data.generator(url.list,21))
length(main.data.generator(url.list,20))
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
if (length(main.data) != 1){
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
}
return(data.list)
}
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
dim(data.matrix)
next.link.2 <- get.next.link2(browser)
browser$navigate(next.link.2)
url.list <- create.thread.list(browser)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
#STEP 8: GO TO THE NEXT PAGE USING THE SECOND "NEXTLINK" FUNCTION AND GO TO NEXT PAGE
next.link.2 <- get.next.link2(browser)
browser$navigate(next.link.2)
reddit.data <- as.data.frame(data.matrix)
reddit.data <- reddit.data[-1,]
names(reddit.data) <- c("Body","Author")
head(reddit.data)
url <- 'https://www.reddit.com/r/politics/'
browser <- open.page(url)
url.list <- create.thread.list(browser)
next.link <- get.next.link(browser)
url.list <- create.thread.list(browser)
next.link.2 <- get.next.link2(browser)
browser$navigate(next.link.2)
