send <- browser$findElement(using = 'id', value="nofollow next")
links
library(rjson)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
url <- 'https://www.reddit.com/r/politics/'
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
}
open.page(url)
create.thread.list <- function(){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
next.link <- urls[grep("?count=25")]
}
next.link
urls
create.thread.list <- function(){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
next.link <- urls[grep("?count=25")]
}
create.thread.list
create.thread.list()
links <- browser$findElements(using="css selector", value=".may-blank")
links <- browser$findElements(using="css selector", value=".may-blank")
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
open.page(url)
links <- browser$findElements(using="css selector", value=".may-blank")
browser <- open.page(url)
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
url.list <- urls[grep("r/politics/comments",urls)]
next.link <- urls[grep("?count=25")]
next.link <- urls[grep("?count=25"),urls]
next.link <- urls[grep("?count=25",urls)]
next.link
urls
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links))
urls.next
links.next
urls.next <- rep(NA, length(links.next))
urls.next
urls <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls[grep("?count=25",urls)]
next.link
#create thread list
create.thread.list <- function(){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
#get the "next" link to crawl later
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls)]
return(url.list,next.link)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
#get the "next" link to crawl later
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls)]
return(url.list,next.link)
}
url <- 'https://www.reddit.com/r/politics/'
browser <- open.page(url)
links <- create.thread.list(browser)
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls)]
return(next.link)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
return(url.list)
}
links <- create.thread.list(browser)
url.list <- create.thread.list(browser)
next.link <- get.next.link(browser)
next.link
url.list
browser$navigate(next.link)
url.list <- create.thread.list(browser)
url.list
next.link <- get.next.link(browser)
browser$navigate(next.link)
next.link <- get.next.link(browser)
next.link
browser$navigate(next.link)
next.link <- get.next.link(browser)
next.link
browser$navigate(next.link)
next.link <- get.next.link(browser)
nextlink
next.link
browser$navigate(next.link)
next.link <- get.next.link(browser)
browser$navigate(next.link)
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value=".separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls)]
return(next.link)
}
next.link.2 <- get.next.link2(browser)
next.link.2
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls)]
return(next.link)
}
next.link.2 <- get.next.link2(browser)
next.link.2
browser$navigate(next.link2)
browser$navigate(next.link.2)
next.link.2 <- get.next.link2(browser)
browser$navigate(next.link.2)
library(rjson)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
#the main URL to pull from
url <- 'https://www.reddit.com/r/politics/'
#open the page
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
browser <- open.page(url)
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
return(url.list)
}
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls)]
return(next.link)
}
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls)]
return(next.link)
}
#convert all comment urls to json
url.to.json <- function(url.list){
for (i in 1:length(url.list)) {
url.list[i] <- paste0(url.list[i],".json")
}
return(url.list)
}
main.data.generator <- function(url.list,i){
#select the URL of the thread to scrape from
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
return(main.data)
#main.data[[5]]$data$body #comment number 5
#main.data[[5]]$data$author #author of comment number 5
#information about the parent post title and url can be found in rawdat[[1]]
}
#this function puts comments and their authors into a matrix from a given main.data
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
#add the other rows
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
return(data.list)
}
url <- 'https://www.reddit.com/r/politics/'
browser <- open.page(url)
#STEP 2: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#STEP 3: INITIALIZE DATA MATRIX
data.matrix <- matrix(c("a","b"),nrow=1)
#STEP 4: GET URLS TO THREADS ON PAGE
url.list <- create.thread.list(browser)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
library(rjson)
library(RJSONIO)
library(rjsonlite)
#STEP 5: GATHER COMMENT DATA FROM EACH THREAD
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url.list
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
for (i in 1:length(url.list)) {
url.list[i] <- paste0(url.list[i],".json")
}
return(url.list)
}
url.list <- create.thread.list(browser)
url.list
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url
url
url.list[1]
url.comment <- as.character(url.list[i])
i <- 1
url.comment <- as.character(url.list[i])
rawdat <- fromJSON(file=url.comment)
i <- 5
url.comment <- as.character(url.list[i])
rawdat <- fromJSON(file=url.comment)
url.comment
?fromJSON
url.comment
rawdat <- fromJSON(file=url.comment)
library(jsonlite)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url.comment
rawdat <- fromJSON(file=url.comment)
?fromJSON
library(rjson)
rawdat <- fromJSON(file=url.comment)
library(rjson)
rawdat <- fromJSON(file=url.comment)
#library(tm)           # Framework for text mining.
#library(SnowballC)    # Provides wordStem() for stemming.
#library(qdap)         # Quantitative discourse analysis of transcripts.
#library(qdapDictionaries)
#library(dplyr)        # Data preparation and pipes %>%.
library(rjson)
#library(RJSONIO)
library(rvest, warn.conflicts=FALSE)
library(RSelenium)
#library(jsonlite)
url <- 'https://www.reddit.com/r/politics/'
#open the page
open.page <- function(url){
checkForServer() # check if Selenium Server package is installed and if not, install now
startServer() # start server
browser <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
browser$open()
browser$navigate(url)
return(browser)
}
#create thread list
create.thread.list <- function(browser){ #works from open browser
links <- browser$findElements(using="css selector", value=".may-blank")
urls <- rep(NA, length(links))
urls <- unlist(sapply(links, function(x) x$getElementAttribute('href')))
#only get the comment urls
url.list <- urls[grep("r/politics/comments",urls)]
for (i in 1:length(url.list)) {
url.list[i] <- paste0(url.list[i],".json")
}
return(url.list)
}
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls)]
return(next.link)
}
#get the "next" link to crawl later AFTER NEXT HAS BEEN CLICKED ONCE
get.next.link2 <- function(browser){
links.next <- browser$findElements(using="css selector", value="#siteTable .separator+ a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=",urls)]
return(next.link)
}
main.data.generator <- function(url.list,i){
#select the URL of the thread to scrape from
url.comment <- as.character(url.list[i])
#convert from JSON to R-usable data
rawdat <- fromJSON(file=url.comment)
#pulling just want we want
main.data <- rawdat[[2]]$data$children
return(main.data)
#main.data[[5]]$data$body #comment number 5
#main.data[[5]]$data$author #author of comment number 5
#information about the parent post title and url can be found in rawdat[[1]]
}
#this function puts comments and their authors into a matrix from a given main.data
comments.to.dataframe <- function(main.data){
#initialize a matrix
data.list <- matrix(c(main.data[[1]]$data$body,main.data[[1]]$data$author),nrow=1)
#add the other rows
for (i in 2:(length(main.data)-1)){
data.list <- rbind(data.list,c(main.data[[i]]$data$body,main.data[[i]]$data$author))
}
return(data.list)
}
url <- 'https://www.reddit.com/r/politics/'
browser <- open.page(url)
#STEP 2: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#STEP 2: GET URLS TO THREADS ON PAGE
url.list <- create.thread.list(browser)
#STEP 4: GET LINK TO CLICK TO NEXT PAGE
next.link <- get.next.link(browser)
#get the "next" link to crawl later
get.next.link <- function(browser){
links.next <- browser$findElements(using="css selector", value=".nextprev a")
urls.next <- rep(NA, length(links.next))
urls.next <- unlist(sapply(links.next, function(x) x$getElementAttribute('href')))
next.link <- urls.next[grep("?count=25",urls.next)]
return(next.link)
}
next.link <- get.next.link(browser)
data.matrix <- matrix(c("a","b"),nrow=1)
for (url in 1:length(url.list)){ #for every thread
main.data <- main.data.generator(url.list,url) #create main data
if (length(main.data) == 0){
next
}
else{
data.matrix <- rbind(data.matrix,comments.to.dataframe(main.data))
}
}
url.list[1]
i <- 6
url.list[i]
url.comment <- as.character(url.list[i])
rawdat <- fromJSON(file=url.comment)
sessionInfo()
